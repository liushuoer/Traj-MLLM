{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def consolidate_porto_anomaly_images(traj_id_str):\n",
    "    \"\"\"\n",
    "    Consolidate all related anomaly detection images for the specified trajectory_id\n",
    "    into a new subdirectory and rename files according to rules.\n",
    "    Returns (target directory path, ordered file list)\n",
    "\n",
    "    Args:\n",
    "        traj_id_str: Trajectory ID string\n",
    "    Returns:\n",
    "        tuple: (target directory path, ordered file list)\n",
    "    \"\"\"\n",
    "    # Base path configuration\n",
    "    base_dir = '../data'\n",
    "    target_consolidation_dir = f\"../consolidated_data/consolidated_{traj_id_str}\"\n",
    "\n",
    "    # Source directory configuration\n",
    "    source_configurations = [\n",
    "        {\n",
    "            \"name_prefix\": \"poi\",\n",
    "            \"source_dir\": os.path.join(base_dir, \"poi_trajectory_images\"),\n",
    "            \"pattern\": rf\"^{traj_id_str}(_segment_\\d+)?\\.png$\"  # Match main and segment images\n",
    "        },\n",
    "        {\n",
    "            \"name_prefix\": \"road_structure\",\n",
    "            \"source_dir\": os.path.join(base_dir, \"road_structure_images\"),\n",
    "            \"pattern\": rf\"^{traj_id_str}(_segment_\\d+)?\\.png$\"  # Match main and segment images\n",
    "        },\n",
    "        {\n",
    "            \"name_prefix\": \"last_trajectory\",\n",
    "            \"source_dir\": os.path.join(base_dir, \"last_trajectory_images\"),\n",
    "            \"pattern\": rf\"^trajectory_{traj_id_str}_\\d+\\.png$\"  # Match images in last_trajectory_images\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Check if target directory already exists\n",
    "    if os.path.exists(target_consolidation_dir):\n",
    "        return None, []\n",
    "\n",
    "    # Create target directory\n",
    "    try:\n",
    "        os.makedirs(target_consolidation_dir, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directory '{target_consolidation_dir}': {e}\")\n",
    "        return None, []\n",
    "\n",
    "    copied_files = []  # Store copied file information\n",
    "\n",
    "    for config in source_configurations:\n",
    "        source_dir_abs = config[\"source_dir\"]\n",
    "        file_source_prefix = config[\"name_prefix\"]\n",
    "        pattern = config[\"pattern\"]\n",
    "\n",
    "        if not os.path.isdir(source_dir_abs):\n",
    "            print(f\"Source directory does not exist: {source_dir_abs}\")\n",
    "            continue\n",
    "\n",
    "        # Use different processing logic based on different source directories\n",
    "        if file_source_prefix == \"last_trajectory\":\n",
    "            # Process last_trajectory_images folder\n",
    "            for original_filename in os.listdir(source_dir_abs):\n",
    "                # Match files like trajectory_04d6595f4e9fb466dade447a6989445c_17.png\n",
    "                last_traj_pattern = re.compile(rf\"^trajectory_{traj_id_str}_(\\d+)\\.png$\")\n",
    "                match = last_traj_pattern.match(original_filename)\n",
    "\n",
    "                if match:\n",
    "                    # Extract sequence number\n",
    "                    sequence_num = match.group(1)\n",
    "                    # Rename to last_trajectory_123_17.png format\n",
    "                    target_filename = f\"last_trajectory_{traj_id_str}_{sequence_num}.png\"\n",
    "\n",
    "                    source_file_path = os.path.join(source_dir_abs, original_filename)\n",
    "                    target_file_path = os.path.join(target_consolidation_dir, target_filename)\n",
    "\n",
    "                    try:\n",
    "                        shutil.copy2(source_file_path, target_file_path)\n",
    "                        copied_files.append({\n",
    "                            \"original_path\": source_file_path,\n",
    "                            \"target_path\": target_file_path,\n",
    "                            \"filename\": target_filename\n",
    "                        })\n",
    "                        print(f\"Copied '{original_filename}' to '{target_filename}'\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error copying '{source_file_path}' to '{target_file_path}': {e}\")\n",
    "        else:\n",
    "            # Process original poi_anomaly_images and road_structure_anomaly_images\n",
    "            for original_filename in os.listdir(source_dir_abs):\n",
    "                target_filename = None  # Initialize target filename\n",
    "                should_copy = False\n",
    "\n",
    "                # Try to match segment image filename, e.g., \"123_segment_0.png\"\n",
    "                segment_pattern = re.compile(rf\"^{traj_id_str}_segment_(\\d+)\\.png$\")\n",
    "                match = segment_pattern.match(original_filename)\n",
    "\n",
    "                if match:\n",
    "                    # Is segment image, rename to 0_segment_123_poi_anomaly.png format\n",
    "                    segment_index = match.group(1)  # Extract index number, e.g., \"0\"\n",
    "                    target_filename = f\"{segment_index}_segment_{traj_id_str}_{file_source_prefix}.png\"\n",
    "                    should_copy = True\n",
    "                elif original_filename == f\"{traj_id_str}.png\":\n",
    "                    # Is main image\n",
    "                    target_filename = f\"{file_source_prefix}_{traj_id_str}.png\"\n",
    "                    should_copy = True\n",
    "\n",
    "                if should_copy and target_filename:\n",
    "                    source_file_path = os.path.join(source_dir_abs, original_filename)\n",
    "                    target_file_path = os.path.join(target_consolidation_dir, target_filename)\n",
    "\n",
    "                    try:\n",
    "                        shutil.copy2(source_file_path, target_file_path)\n",
    "                        copied_files.append({\n",
    "                            \"original_path\": source_file_path,\n",
    "                            \"target_path\": target_file_path,\n",
    "                            \"filename\": target_filename\n",
    "                        })\n",
    "                        print(f\"Copied '{original_filename}' to '{target_filename}'\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error copying '{source_file_path}' to '{target_file_path}': {e}\")\n",
    "\n",
    "    # Sort filenames\n",
    "    copied_files.sort(key=lambda x: x[\"filename\"])\n",
    "    file_list = [item[\"filename\"] for item in copied_files]\n",
    "\n",
    "    # Save file_list.pkl\n",
    "    if copied_files:\n",
    "        pickle_file_path = os.path.join(target_consolidation_dir, 'file_list.pkl')\n",
    "        try:\n",
    "            with open(pickle_file_path, 'wb') as pickle_file:\n",
    "                pickle.dump(file_list, pickle_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving file list: {e}\")\n",
    "\n",
    "        # Print statistics of copied files\n",
    "        file_types = defaultdict(int)\n",
    "        for item in copied_files:\n",
    "            filename = item[\"filename\"]\n",
    "            if \"poi\" in filename:\n",
    "                file_types[\"poi\"] += 1\n",
    "            elif \"road_structure\" in filename:\n",
    "                file_types[\"road_structure\"] += 1\n",
    "            elif \"last_trajectory\" in filename:\n",
    "                file_types[\"last_trajectory\"] += 1\n",
    "\n",
    "        print(f\"\\nCopy statistics - Trajectory {traj_id_str}:\")\n",
    "        print(f\"Total: {len(copied_files)} files\")\n",
    "        for file_type, count in file_types.items():\n",
    "            print(f\"- {file_type}: {count} files\")\n",
    "\n",
    "        return target_consolidation_dir, file_list\n",
    "    else:\n",
    "        print(f\"Warning: No files found related to trajectory ID '{traj_id_str}'\")\n",
    "        return target_consolidation_dir, []"
   ],
   "id": "f013f05073e69331",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_trajectory_ids_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Extract all unique trajectory IDs from segment images in the specified folder,\n",
    "    excluding system-generated hidden files\n",
    "\n",
    "    Args:\n",
    "        folder_path: Folder path\n",
    "    Returns:\n",
    "        list: List of unique trajectory IDs\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Folder does not exist: {folder_path}\")\n",
    "        return []\n",
    "\n",
    "    traj_ids = set()\n",
    "\n",
    "    # Only extract segment image filename format: \"traj_id_segment_n.png\", e.g., \"107.0_segment_0.png\"\n",
    "    segment_pattern = re.compile(r\"^([^._][^/\\\\:*?\\\"<>|]*)_segment_\\d+\\.png$\")\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Skip hidden files and files starting with ._\n",
    "        if filename.startswith('.') or filename.startswith('._'):\n",
    "            continue\n",
    "\n",
    "        if not filename.endswith('.png'):\n",
    "            continue\n",
    "\n",
    "        # Check if it's segment image format\n",
    "        segment_match = segment_pattern.match(filename)\n",
    "        if segment_match:\n",
    "            traj_id = segment_match.group(1)  # e.g., extract \"107.0\" from \"107.0_segment_0.png\"\n",
    "            traj_ids.add(traj_id)\n",
    "\n",
    "    return list(traj_ids)"
   ],
   "id": "d9fe4bc5cd75fea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_dir = '../data'\n",
    "poi_folder = os.path.join(base_dir, \"poi_trajectory_images\")\n",
    "road_folder = os.path.join(base_dir, \"road_structure_images\")\n",
    "poi_traj_ids = get_trajectory_ids_from_folder(poi_folder)\n",
    "road_traj_ids = get_trajectory_ids_from_folder(road_folder)\n"
   ],
   "id": "2a57ebe9c867f9b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Merged trajectory IDs\n",
    "all_traj_ids = list(set(poi_traj_ids + road_traj_ids))\n",
    "print(f\"Extracted {len(all_traj_ids)} unique trajectory IDs from folders\")\n",
    "\n",
    "# Parallel processing function\n",
    "def process_trajectory(traj_id):\n",
    "    try:\n",
    "        target_dir, file_list = consolidate_porto_anomaly_images(traj_id)\n",
    "        if file_list:\n",
    "            return traj_id, True  # Indicates success\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing trajectory ID {traj_id}: {e}\")\n",
    "    return traj_id, False  # Indicates failure\n",
    "\n",
    "# Use multi-threading for parallel processing\n",
    "success_count = 0\n",
    "max_workers = 8  # Set number of threads, can be adjusted based on machine\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(process_trajectory, traj_id): traj_id for traj_id in all_traj_ids}\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing trajectories\"):\n",
    "        traj_id, success = future.result()\n",
    "        if success:\n",
    "            success_count += 1\n",
    "\n",
    "print(f\"Processing completed. Successfully consolidated images for {success_count}/{len(all_traj_ids)} trajectories.\")"
   ],
   "id": "39ca81b753ff80e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f986ab056e126872",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
